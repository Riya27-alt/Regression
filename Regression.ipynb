{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                        REGRESSION ASESSMENT\n",
        "1. What is Simple Linear Regression?\n",
        "It is a statistical technique that models the relationship between one independent variable (X) and one dependent variable (Y) using a straight-line equation:\n",
        "Y = mX + c\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity between X and Y\n",
        "\n",
        "Independence of errors\n",
        "\n",
        "Homoscedasticity (constant variance of errors)\n",
        "\n",
        "Normal distribution of residuals\n",
        "\n",
        "3. What does the coefficient m represent in Y = mX + c?\n",
        "The slope (m) represents the rate of change in Y for a one-unit increase in X. It indicates the strength and direction of the relationship.\n",
        "\n",
        "4. What does the intercept c represent in Y = mX + c?\n",
        "The intercept (c) is the value of Y when X = 0. It’s the starting point of the line on the Y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "𝑚\n",
        "=\n",
        "𝑛\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "(\n",
        "∑\n",
        "𝑌\n",
        ")\n",
        "𝑛\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "2\n",
        "m=\n",
        "n∑X\n",
        "2\n",
        " −(∑X)\n",
        "2\n",
        "\n",
        "n∑XY−(∑X)(∑Y)\n",
        "​\n",
        "\n",
        "6. What is the purpose of the least squares method in SLR?\n",
        "To find the best-fitting line by minimizing the sum of the squares of the residuals (differences between actual and predicted Y values).\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in SLR?\n",
        "R² explains the proportion of variance in Y explained by X.\n",
        "For example, R² = 0.80 means 80% of the variation in Y is explained by X.\n",
        "\n",
        "🔹 Multiple Linear Regression (MLR)\n",
        "8. What is Multiple Linear Regression?\n",
        "It’s a method to model the relationship between two or more independent variables and one dependent variable:\n",
        "Y = b₀ + b₁X₁ + b₂X₂ + … + bₙXₙ\n",
        "\n",
        "9. What is the main difference between SLR and MLR?\n",
        "SLR has one independent variable; MLR has two or more.\n",
        "\n",
        "10. What are the key assumptions of MLR?\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "No multicollinearity among predictors\n",
        "\n",
        "11. What is heteroscedasticity and how does it affect MLR results?\n",
        "It means unequal variance of residuals. It violates assumptions and can make standard errors unreliable, affecting significance tests.\n",
        "\n",
        "12. How can you improve an MLR model with high multicollinearity?\n",
        "\n",
        "Remove or combine correlated predictors\n",
        "\n",
        "Use dimensionality reduction like PCA\n",
        "\n",
        "Use Ridge or Lasso regression\n",
        "\n",
        "13. What are common techniques for transforming categorical variables for regression?\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Creating dummy variables\n",
        "\n",
        "14. What is the role of interaction terms in MLR?\n",
        "They allow modeling of the combined effect of two variables on the outcome.\n",
        "Example: Y = b₀ + b₁X + b₂Z + b₃(X×Z)\n",
        "\n",
        "15. How can the interpretation of intercept differ in SLR and MLR?\n",
        "In SLR: Y when X = 0\n",
        "In MLR: Y when all Xs = 0, which may not always make practical sense.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis?\n",
        "It indicates how much Y is expected to increase (or decrease) when a predictor increases by one unit.\n",
        "\n",
        "17. How does the intercept provide context for the relationship?\n",
        "It gives the base value of the outcome when all predictors are 0. It helps interpret the overall trend.\n",
        "\n",
        "18. What are the limitations of using R² alone to evaluate performance?\n",
        "\n",
        "Increases with more variables (even irrelevant ones)\n",
        "\n",
        "Doesn’t tell if predictors are meaningful\n",
        "\n",
        "Doesn’t reveal overfitting or model bias\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "It means the coefficient is less reliable or statistically insignificant. The variable may not strongly affect Y.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots? Why is it important to address?\n",
        "By plotting residuals vs. predicted values — a cone or pattern indicates heteroscedasticity. It can distort predictions and confidence intervals.\n",
        "\n",
        "21. What does it mean if a model has high R² but low adjusted R²?\n",
        "It suggests the model includes irrelevant variables that don’t improve prediction — possibly overfitting.\n",
        "\n",
        "22. Why is it important to scale variables in MLR?\n",
        "Because predictors on different scales can distort the model and affect optimization. It’s crucial for regularized regression (like Ridge).\n",
        "\n",
        "🔹 Polynomial Regression\n",
        "23. What is Polynomial Regression?\n",
        "A form of regression where the relationship between X and Y is modeled as an nth-degree polynomial:\n",
        "Y = b₀ + b₁X + b₂X² + … + bₙXⁿ\n",
        "\n",
        "24. How does Polynomial Regression differ from Linear Regression?\n",
        "Linear regression models a straight-line relationship, while polynomial regression models curved relationships.\n",
        "\n",
        "25. When is Polynomial Regression used?\n",
        "When the data shows a non-linear pattern that cannot be captured by a straight line.\n",
        "\n",
        "26. What is the general equation for Polynomial Regression?\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "…\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +…+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "27. Can Polynomial Regression be applied to multiple variables?\n",
        "Yes. You can include polynomial terms and interaction terms of multiple variables.\n",
        "\n",
        "28. What are the limitations of Polynomial Regression?\n",
        "\n",
        "Overfitting (if degree is too high)\n",
        "\n",
        "Hard to interpret\n",
        "\n",
        "Sensitive to outliers\n",
        "\n",
        "Poor extrapolation\n",
        "\n",
        "29. What methods can be used to select the degree of a polynomial?\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "R² and Adjusted R²\n",
        "\n",
        "AIC/BIC\n",
        "\n",
        "Residual analysis\n",
        "\n",
        "30. Why is visualization important in Polynomial Regression?\n",
        "It helps detect overfitting, assess how well the curve fits the data, and improve model interpretability.\n",
        "\n",
        "31. How is Polynomial Regression implemented in Python?\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a3G51jlfPpi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Bcvb_WivPjky",
        "outputId": "fb328982-1f67-4b94-a251-c2bd049d7c24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bcc7a851cb2f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sample data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Transform to polynomial features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 6, 14, 28, 45])\n",
        "\n",
        "# Transform to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}